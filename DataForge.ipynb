{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz0HuXI/MwR/8Fq3mTWl0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Peagledor/ai-projects/blob/main/DataForge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers gradio pandas\n",
        "!python your_script.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rArnGQ4DpRf",
        "outputId": "a76dde3d-f755-4a2d-d9e5-453fb1e543bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.13)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.12.0 gradio-client-1.5.4 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "python3: can't open file '/content/your_script.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TextStreamer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_llama_model(self):\n",
        "        \"\"\"Load the LLaMA 3.1B model\"\"\"\n",
        "        try:\n",
        "            print(\"Starting model loading process...\")\n",
        "            # Use the same model as in your working example\n",
        "            model_name = \"meta-llama/Llama-2-3.1b-chat-hf\"\n",
        "            print(f\"Attempting to load tokenizer for {model_name}\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            print(\"Tokenizer loaded successfully\")\n",
        "\n",
        "            # Configure quantization like in the PDF example\n",
        "            quant_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                bnb_4bit_quant_type=\"nf4\"\n",
        "            )\n",
        "\n",
        "            print(\"Starting model loading...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                quantization_config=quant_config\n",
        "            )\n",
        "            print(\"Model loaded successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Detailed error loading LLaMA model: {str(e)}\")\n",
        "            print(f\"Error type: {type(e)}\")\n",
        "            import traceback\n",
        "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def generate_with_model(self, prompt, num_records):\n",
        "        \"\"\"Generate data using the loaded model\"\"\"\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a data generation assistant. Generate realistic test data in JSON format.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Generate {num_records} records of {prompt} in JSON format.\"}\n",
        "            ]\n",
        "\n",
        "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=2000,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "            # Extract JSON from response\n",
        "            try:\n",
        "                # Find JSON content within the response\n",
        "                start_idx = response.find('[')\n",
        "                end_idx = response.rfind(']') + 1\n",
        "                if start_idx != -1 and end_idx != -1:\n",
        "                    json_str = response[start_idx:end_idx]\n",
        "                    data = json.loads(json_str)\n",
        "                    return pd.DataFrame(data)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            return self.generate_template_based(prompt, num_records)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in model generation: {str(e)}\")\n",
        "            return self.generate_template_based(prompt, num_records)\n",
        "\n",
        "    def generate_template_based(self, description, num_records):\n",
        "        \"\"\"Generate data using templates and customization\"\"\"\n",
        "        desc_lower = description.lower()\n",
        "\n",
        "        # Detect business type from description\n",
        "        if \"retail\" in desc_lower or \"product\" in desc_lower or \"store\" in desc_lower:\n",
        "            return self._generate_retail_data(num_records, description)\n",
        "        elif \"health\" in desc_lower or \"medical\" in desc_lower or \"patient\" in desc_lower:\n",
        "            return self._generate_healthcare_data(num_records, description)\n",
        "        elif \"finance\" in desc_lower or \"bank\" in desc_lower or \"transaction\" in desc_lower:\n",
        "            return self._generate_finance_data(num_records, description)\n",
        "        elif \"tech\" in desc_lower or \"software\" in desc_lower or \"user\" in desc_lower:\n",
        "            return self._generate_tech_data(num_records, description)\n",
        "        else:\n",
        "            return self._generate_custom_data(num_records, description)\n",
        "\n",
        "    def _generate_retail_data(self, num_records, description):\n",
        "        \"\"\"Generate retail business data with customization\"\"\"\n",
        "        # Parse description for customization hints\n",
        "        include_inventory = \"inventory\" in description.lower()\n",
        "        include_customer = \"customer\" in description.lower()\n",
        "\n",
        "        data = {\n",
        "            'transaction_id': [f'TRX{i:06d}' for i in range(num_records)],\n",
        "            'date': [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n",
        "                    for i in range(num_records)],\n",
        "            'product_id': [f'PRD{i:04d}' for i in range(num_records)],\n",
        "            'product_name': [f'Product {i}' for i in range(num_records)],\n",
        "            'quantity': [random.randint(1, 100) for _ in range(num_records)],\n",
        "            'unit_price': [round(random.uniform(10.0, 1000.0), 2) for _ in range(num_records)]\n",
        "        }\n",
        "\n",
        "        if include_inventory:\n",
        "            data['stock_level'] = [random.randint(0, 1000) for _ in range(num_records)]\n",
        "            data['reorder_point'] = [random.randint(10, 100) for _ in range(num_records)]\n",
        "\n",
        "        if include_customer:\n",
        "            data['customer_id'] = [f'CUST{random.randint(1000, 9999)}' for _ in range(num_records)]\n",
        "            data['customer_segment'] = [random.choice(['Regular', 'Premium', 'VIP'])\n",
        "                                      for _ in range(num_records)]\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_healthcare_data(self, num_records, description):\n",
        "        \"\"\"Generate healthcare data with customization\"\"\"\n",
        "        departments = ['Cardiology', 'Neurology', 'Pediatrics', 'Orthopedics', 'Internal Medicine']\n",
        "        data = {\n",
        "            'patient_id': [f'PAT{i:06d}' for i in range(num_records)],\n",
        "            'appointment_date': [(datetime.now() + timedelta(days=i)).strftime('%Y-%m-%d')\n",
        "                               for i in range(num_records)],\n",
        "            'doctor_id': [f'DOC{random.randint(100, 999)}' for _ in range(num_records)],\n",
        "            'department': [random.choice(departments) for _ in range(num_records)]\n",
        "        }\n",
        "\n",
        "        if \"insurance\" in description.lower():\n",
        "            data['insurance_provider'] = [f'INS{random.randint(100, 999)}'\n",
        "                                        for _ in range(num_records)]\n",
        "            data['coverage_type'] = [random.choice(['Full', 'Partial', 'Basic'])\n",
        "                                   for _ in range(num_records)]\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_finance_data(self, num_records, description):\n",
        "        \"\"\"Generate financial data with customization\"\"\"\n",
        "        transaction_types = ['deposit', 'withdrawal', 'transfer', 'payment']\n",
        "        data = {\n",
        "            'transaction_id': [f'FIN{i:06d}' for i in range(num_records)],\n",
        "            'date': [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n",
        "                    for i in range(num_records)],\n",
        "            'type': [random.choice(transaction_types) for _ in range(num_records)],\n",
        "            'amount': [round(random.uniform(10.0, 10000.0), 2) for _ in range(num_records)]\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_tech_data(self, num_records, description):\n",
        "        \"\"\"Generate technology-related data with customization\"\"\"\n",
        "        status_options = ['active', 'inactive', 'pending', 'completed']\n",
        "        data = {\n",
        "            'event_id': [f'TECH{i:06d}' for i in range(num_records)],\n",
        "            'timestamp': [(datetime.now() - timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        for i in range(num_records)],\n",
        "            'status': [random.choice(status_options) for _ in range(num_records)],\n",
        "            'user_id': [f'USR{random.randint(1000, 9999)}' for _ in range(num_records)]\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_custom_data(self, num_records, description):\n",
        "        \"\"\"Generate custom data based on description\"\"\"\n",
        "        # Basic implementation - this could be enhanced based on description parsing\n",
        "        data = {\n",
        "            'id': [f'CUSTOM_{i:06d}' for i in range(num_records)],\n",
        "            'timestamp': [(datetime.now() - timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        for i in range(num_records)],\n",
        "            'value': [round(random.uniform(0, 1000), 2) for _ in range(num_records)]\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "class DataForgeUI:\n",
        "    def __init__(self):\n",
        "        self.generator = DataGenerator()\n",
        "        self.model_loaded = False\n",
        "\n",
        "    def create_interface(self):\n",
        "        with gr.Blocks() as interface:\n",
        "            gr.Markdown(\"# Advanced Data Forge: Synthetic Business Data Generator\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_choice = gr.Radio(\n",
        "                        choices=[\"Template Based\", \"LLaMA Model\"],\n",
        "                        label=\"Generation Method\",\n",
        "                        value=\"Template Based\"\n",
        "                    )\n",
        "\n",
        "                    load_model_btn = gr.Button(\"Load LLaMA Model\")\n",
        "                    model_status = gr.Markdown(\"Model Status: Not Loaded\")\n",
        "\n",
        "            with gr.Row():\n",
        "                description = gr.Textbox(\n",
        "                    label=\"Describe the data you need\",\n",
        "                    placeholder=\"Example: Generate retail data with inventory levels and customer segments\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                num_records = gr.Slider(\n",
        "                    minimum=5,\n",
        "                    maximum=100,\n",
        "                    value=10,\n",
        "                    step=5,\n",
        "                    label=\"Number of Records\"\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                generate_btn = gr.Button(\"Generate Data\")\n",
        "\n",
        "            with gr.Row():\n",
        "                output_table = gr.DataFrame()\n",
        "\n",
        "            # Example templates\n",
        "            gr.Markdown(\"### Example Templates\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"Generate retail data with inventory levels and customer segments\", 10],\n",
        "                    [\"Create healthcare records with insurance information\", 15],\n",
        "                    [\"Generate financial transaction data with different account types\", 20],\n",
        "                    [\"Create technology usage logs with user activity\", 25]\n",
        "                ],\n",
        "                inputs=[description, num_records]\n",
        "            )\n",
        "\n",
        "            def load_model():\n",
        "                success = self.generator.load_llama_model()\n",
        "                self.model_loaded = success\n",
        "                return \"Model Status: Loaded Successfully\" if success else \"Model Status: Loading Failed\"\n",
        "\n",
        "            def generate_data(description, num_records, model_choice):\n",
        "                if model_choice == \"LLaMA Model\" and self.model_loaded:\n",
        "                    return self.generator.generate_with_model(description, num_records)\n",
        "                else:\n",
        "                    return self.generator.generate_template_based(description, num_records)\n",
        "\n",
        "            load_model_btn.click(\n",
        "                fn=load_model,\n",
        "                outputs=[model_status]\n",
        "            )\n",
        "\n",
        "            generate_btn.click(\n",
        "                fn=generate_data,\n",
        "                inputs=[description, num_records, model_choice],\n",
        "                outputs=[output_table]\n",
        "            )\n",
        "\n",
        "        return interface\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ui = DataForgeUI()\n",
        "    interface = ui.create_interface()\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "ghPKY91LAw8W",
        "outputId": "04576a03-4e89-40ee-b91c-8543f374aff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://80d9a7fcbe42383175.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://80d9a7fcbe42383175.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sy0q9YvWCROJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}