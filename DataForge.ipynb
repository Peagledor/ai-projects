{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz0HuXI/MwR/8Fq3mTWl0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Peagledor/ai-projects/blob/main/DataForge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers gradio pandas\n",
        "!python your_script.py"
      ],
      "metadata": {
        "id": "2rArnGQ4DpRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TextStreamer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_llama_model(self):\n",
        "        \"\"\"Load the LLaMA 3.1B model\"\"\"\n",
        "        try:\n",
        "            print(\"Starting model loading process...\")\n",
        "            # Use the same model as in your working example\n",
        "            model_name = \"meta-llama/Llama-2-3.1b-chat-hf\"\n",
        "            print(f\"Attempting to load tokenizer for {model_name}\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            print(\"Tokenizer loaded successfully\")\n",
        "\n",
        "            # Configure quantization like in the PDF example\n",
        "            quant_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                bnb_4bit_quant_type=\"nf4\"\n",
        "            )\n",
        "\n",
        "            print(\"Starting model loading...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                quantization_config=quant_config\n",
        "            )\n",
        "            print(\"Model loaded successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Detailed error loading LLaMA model: {str(e)}\")\n",
        "            print(f\"Error type: {type(e)}\")\n",
        "            import traceback\n",
        "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def generate_with_model(self, prompt, num_records):\n",
        "        \"\"\"Generate data using the loaded model\"\"\"\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a data generation assistant. Generate realistic test data in JSON format.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Generate {num_records} records of {prompt} in JSON format.\"}\n",
        "            ]\n",
        "\n",
        "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=2000,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "            # Extract JSON from response\n",
        "            try:\n",
        "                # Find JSON content within the response\n",
        "                start_idx = response.find('[')\n",
        "                end_idx = response.rfind(']') + 1\n",
        "                if start_idx != -1 and end_idx != -1:\n",
        "                    json_str = response[start_idx:end_idx]\n",
        "                    data = json.loads(json_str)\n",
        "                    return pd.DataFrame(data)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            return self.generate_template_based(prompt, num_records)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in model generation: {str(e)}\")\n",
        "            return self.generate_template_based(prompt, num_records)\n",
        "\n",
        "    def generate_template_based(self, description, num_records):\n",
        "        \"\"\"Generate data using templates and customization\"\"\"\n",
        "        desc_lower = description.lower()\n",
        "\n",
        "        # Detect business type from description\n",
        "        if \"retail\" in desc_lower or \"product\" in desc_lower or \"store\" in desc_lower:\n",
        "            return self._generate_retail_data(num_records, description)\n",
        "        elif \"health\" in desc_lower or \"medical\" in desc_lower or \"patient\" in desc_lower:\n",
        "            return self._generate_healthcare_data(num_records, description)\n",
        "        elif \"finance\" in desc_lower or \"bank\" in desc_lower or \"transaction\" in desc_lower:\n",
        "            return self._generate_finance_data(num_records, description)\n",
        "        elif \"tech\" in desc_lower or \"software\" in desc_lower or \"user\" in desc_lower:\n",
        "            return self._generate_tech_data(num_records, description)\n",
        "        else:\n",
        "            return self._generate_custom_data(num_records, description)\n",
        "\n",
        "    def _generate_retail_data(self, num_records, description):\n",
        "        \"\"\"Generate retail business data with customization\"\"\"\n",
        "        # Parse description for customization hints\n",
        "        include_inventory = \"inventory\" in description.lower()\n",
        "        include_customer = \"customer\" in description.lower()\n",
        "\n",
        "        data = {\n",
        "            'transaction_id': [f'TRX{i:06d}' for i in range(num_records)],\n",
        "            'date': [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n",
        "                    for i in range(num_records)],\n",
        "            'product_id': [f'PRD{i:04d}' for i in range(num_records)],\n",
        "            'product_name': [f'Product {i}' for i in range(num_records)],\n",
        "            'quantity': [random.randint(1, 100) for _ in range(num_records)],\n",
        "            'unit_price': [round(random.uniform(10.0, 1000.0), 2) for _ in range(num_records)]\n",
        "        }\n",
        "\n",
        "        if include_inventory:\n",
        "            data['stock_level'] = [random.randint(0, 1000) for _ in range(num_records)]\n",
        "            data['reorder_point'] = [random.randint(10, 100) for _ in range(num_records)]\n",
        "\n",
        "        if include_customer:\n",
        "            data['customer_id'] = [f'CUST{random.randint(1000, 9999)}' for _ in range(num_records)]\n",
        "            data['customer_segment'] = [random.choice(['Regular', 'Premium', 'VIP'])\n",
        "                                      for _ in range(num_records)]\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_healthcare_data(self, num_records, description):\n",
        "        \"\"\"Generate healthcare data with customization\"\"\"\n",
        "        departments = ['Cardiology', 'Neurology', 'Pediatrics', 'Orthopedics', 'Internal Medicine']\n",
        "        data = {\n",
        "            'patient_id': [f'PAT{i:06d}' for i in range(num_records)],\n",
        "            'appointment_date': [(datetime.now() + timedelta(days=i)).strftime('%Y-%m-%d')\n",
        "                               for i in range(num_records)],\n",
        "            'doctor_id': [f'DOC{random.randint(100, 999)}' for _ in range(num_records)],\n",
        "            'department': [random.choice(departments) for _ in range(num_records)]\n",
        "        }\n",
        "\n",
        "        if \"insurance\" in description.lower():\n",
        "            data['insurance_provider'] = [f'INS{random.randint(100, 999)}'\n",
        "                                        for _ in range(num_records)]\n",
        "            data['coverage_type'] = [random.choice(['Full', 'Partial', 'Basic'])\n",
        "                                   for _ in range(num_records)]\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_finance_data(self, num_records, description):\n",
        "        \"\"\"Generate financial data with customization\"\"\"\n",
        "        transaction_types = ['deposit', 'withdrawal', 'transfer', 'payment']\n",
        "        data = {\n",
        "            'transaction_id': [f'FIN{i:06d}' for i in range(num_records)],\n",
        "            'date': [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n",
        "                    for i in range(num_records)],\n",
        "            'type': [random.choice(transaction_types) for _ in range(num_records)],\n",
        "            'amount': [round(random.uniform(10.0, 10000.0), 2) for _ in range(num_records)]\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_tech_data(self, num_records, description):\n",
        "        \"\"\"Generate technology-related data with customization\"\"\"\n",
        "        status_options = ['active', 'inactive', 'pending', 'completed']\n",
        "        data = {\n",
        "            'event_id': [f'TECH{i:06d}' for i in range(num_records)],\n",
        "            'timestamp': [(datetime.now() - timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        for i in range(num_records)],\n",
        "            'status': [random.choice(status_options) for _ in range(num_records)],\n",
        "            'user_id': [f'USR{random.randint(1000, 9999)}' for _ in range(num_records)]\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _generate_custom_data(self, num_records, description):\n",
        "        \"\"\"Generate custom data based on description\"\"\"\n",
        "        # Basic implementation - this could be enhanced based on description parsing\n",
        "        data = {\n",
        "            'id': [f'CUSTOM_{i:06d}' for i in range(num_records)],\n",
        "            'timestamp': [(datetime.now() - timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        for i in range(num_records)],\n",
        "            'value': [round(random.uniform(0, 1000), 2) for _ in range(num_records)]\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "class DataForgeUI:\n",
        "    def __init__(self):\n",
        "        self.generator = DataGenerator()\n",
        "        self.model_loaded = False\n",
        "\n",
        "    def create_interface(self):\n",
        "        with gr.Blocks() as interface:\n",
        "            gr.Markdown(\"# Advanced Data Forge: Synthetic Business Data Generator\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_choice = gr.Radio(\n",
        "                        choices=[\"Template Based\", \"LLaMA Model\"],\n",
        "                        label=\"Generation Method\",\n",
        "                        value=\"Template Based\"\n",
        "                    )\n",
        "\n",
        "                    load_model_btn = gr.Button(\"Load LLaMA Model\")\n",
        "                    model_status = gr.Markdown(\"Model Status: Not Loaded\")\n",
        "\n",
        "            with gr.Row():\n",
        "                description = gr.Textbox(\n",
        "                    label=\"Describe the data you need\",\n",
        "                    placeholder=\"Example: Generate retail data with inventory levels and customer segments\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                num_records = gr.Slider(\n",
        "                    minimum=5,\n",
        "                    maximum=100,\n",
        "                    value=10,\n",
        "                    step=5,\n",
        "                    label=\"Number of Records\"\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                generate_btn = gr.Button(\"Generate Data\")\n",
        "\n",
        "            with gr.Row():\n",
        "                output_table = gr.DataFrame()\n",
        "\n",
        "            # Example templates\n",
        "            gr.Markdown(\"### Example Templates\")\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"Generate retail data with inventory levels and customer segments\", 10],\n",
        "                    [\"Create healthcare records with insurance information\", 15],\n",
        "                    [\"Generate financial transaction data with different account types\", 20],\n",
        "                    [\"Create technology usage logs with user activity\", 25]\n",
        "                ],\n",
        "                inputs=[description, num_records]\n",
        "            )\n",
        "\n",
        "            def load_model():\n",
        "                success = self.generator.load_llama_model()\n",
        "                self.model_loaded = success\n",
        "                return \"Model Status: Loaded Successfully\" if success else \"Model Status: Loading Failed\"\n",
        "\n",
        "            def generate_data(description, num_records, model_choice):\n",
        "                if model_choice == \"LLaMA Model\" and self.model_loaded:\n",
        "                    return self.generator.generate_with_model(description, num_records)\n",
        "                else:\n",
        "                    return self.generator.generate_template_based(description, num_records)\n",
        "\n",
        "            load_model_btn.click(\n",
        "                fn=load_model,\n",
        "                outputs=[model_status]\n",
        "            )\n",
        "\n",
        "            generate_btn.click(\n",
        "                fn=generate_data,\n",
        "                inputs=[description, num_records, model_choice],\n",
        "                outputs=[output_table]\n",
        "            )\n",
        "\n",
        "        return interface\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ui = DataForgeUI()\n",
        "    interface = ui.create_interface()\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "ghPKY91LAw8W",
        "outputId": "04576a03-4e89-40ee-b91c-8543f374aff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://80d9a7fcbe42383175.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://80d9a7fcbe42383175.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sy0q9YvWCROJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}